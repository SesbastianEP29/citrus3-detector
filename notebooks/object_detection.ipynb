{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a126e238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access to project 4 verified successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | FasterRCNN | 41.3 M | train\n",
      "---------------------------------------------\n",
      "41.1 M    Trainable params\n",
      "222 K     Non-trainable params\n",
      "41.3 M    Total params\n",
      "165.197   Total estimated model params size (MB)\n",
      "189       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Usuario\\anaconda3\\envs\\camera_trap_workshop\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [01:54<00:00,  0.02it/s, v_num=2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [02:11<00:00,  0.02it/s, v_num=2]\n"
     ]
    }
   ],
   "source": [
    "# Importamos las librerías necesarias\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "import pytorch_lightning as pl\n",
    "from label_studio_sdk import Client\n",
    "from PIL import Image\n",
    "\n",
    "# Definimos un dataset personalizado para cargar las imágenes y anotaciones\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        annotations: lista de anotaciones (provenientes de Label Studio)\n",
    "        img_dir: directorio donde se encuentran las imágenes\n",
    "        transform: transformaciones a aplicar a las imágenes\n",
    "        \"\"\"\n",
    "        self.annotations = annotations\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.label_mapping = self.create_label_mapping()\n",
    "\n",
    "    def create_label_mapping(self):\n",
    "        \"\"\"\n",
    "        Creates a mapping from string labels to numeric values.\n",
    "        \"\"\"\n",
    "        unique_labels = set(annotation['label'] for annotation in self.annotations)\n",
    "        return {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "    def label_to_numeric(self, label):\n",
    "        \"\"\"\n",
    "        Converts a string label to its numeric value.\n",
    "        \"\"\"\n",
    "        return self.label_mapping[label]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image and annotations\n",
    "        annotation = self.annotations[idx]\n",
    "        # Extract the image path\n",
    "        image_name = os.path.basename(annotation['image'].split(\"-\")[-1])\n",
    "        img_path = os.path.join(self.img_dir, image_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Ensure bounding boxes are 2D tensors of shape [N, 4]\n",
    "        boxes = torch.tensor(annotation['bbox'], dtype=torch.float32)\n",
    "        if boxes.ndimension() == 1:  # If it's a single bounding box (1D), reshape to 2D\n",
    "            boxes = boxes.unsqueeze(0)\n",
    "        \n",
    "        # Convert label to tensor\n",
    "        labels = torch.tensor([self.label_to_numeric(annotation['label'])], dtype=torch.int64)  # Wrap label in a list\n",
    "\n",
    "        # Apply transformations to the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Return image and targets as a dictionary\n",
    "        targets = {\"boxes\": boxes, \"labels\": labels}\n",
    "        return image, targets\n",
    "\n",
    "# Definimos el modelo Lightning\n",
    "class ObjectDetectionModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Cargamos un modelo preentrenado de torchvision (Faster R-CNN)\n",
    "        self.model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        # Ajustamos el número de clases\n",
    "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # Guardamos los bound boxes\n",
    "        self.model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        # Forward para entrenamiento o inferencia\n",
    "        return self.model(images, targets)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        loss_dict = self.model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        for key, value in loss_dict.items():\n",
    "            self.log(f\"train_{key}\", value)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Optimizador\n",
    "        return torch.optim.SGD(self.model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Función para cargar anotaciones desde Label Studio\n",
    "def load_annotations_from_label_studio(api_url, api_key, project_id):\n",
    "    client = Client(api_url, api_key)\n",
    "    project = client.get_project(project_id)\n",
    "    tasks = project.get_labeled_tasks()\n",
    "    annotations = []\n",
    "    for task in tasks:\n",
    "        # Extraemos las anotaciones relevantes\n",
    "        bbox_data = task['annotations'][0]['result'][0]['value']\n",
    "        x_min = bbox_data['x']\n",
    "        y_min = bbox_data['y']\n",
    "        x_max = x_min + bbox_data['width']\n",
    "        y_max = y_min + bbox_data['height']\n",
    "        # Equivocacion intentional para practicar\n",
    "        #\"labels\":task['annotations'][0]['result'][0]['value']['labels']\n",
    "        label = task['annotations'][0]['result'][0]['value']['rectanglelabels'][0]\n",
    "\n",
    "        annotations.append({\n",
    "            \"image\": task['data']['image'],\n",
    "            \"bbox\": [x_min, y_min, x_max, y_max],\n",
    "            \"label\":label\n",
    "        })\n",
    "\n",
    "    return annotations\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)  # Unzip the batch\n",
    "    return list(images), list(targets)  # Ensure targets is a list of dicts\n",
    "\n",
    "# Configuración principal\n",
    "if __name__ == \"__main__\":\n",
    "    # Parámetros\n",
    "    api_url = \"http://localhost:8080\"  # URL de Label Studio\n",
    "\n",
    "    # Cargamos la clave de API desde un archivo en la misma carpeta\n",
    "    with open((\"label_studio_key.txt\"), \"r\") as key_file:\n",
    "        api_key = key_file.read().strip()\n",
    "\n",
    "    project_id = 4  # ID del proyecto en Label Studio, cambia esto según tu configuración\n",
    "    \n",
    "    # Verificamos la clave de API y el acceso al proyecto\n",
    "    def verify_api_key(api_url, api_key, project_id):\n",
    "        client = Client(api_url, api_key)\n",
    "        try:\n",
    "            project = client.get_project(project_id)\n",
    "            print(f\"Access to project {project_id} verified successfully.\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to verify API key or project access: {e}\")\n",
    "    # Verificamos el acceso al proyecto\n",
    "    verify_api_key(api_url, api_key, project_id)\n",
    "\n",
    "    ## Cambia esto a tu directorio de imágenes\n",
    "    img_dir = \"D://Cursos//IA for monitoring//IA-sustainability-day-2//example_airborne_birds//\"  # Directorio de imágenes\n",
    "    num_classes = 2  # Número de clases (incluyendo fondo)\n",
    "\n",
    "    # Cargamos las anotaciones\n",
    "    annotations = load_annotations_from_label_studio(api_url, api_key,\n",
    "                                                     project_id)\n",
    "\n",
    "    # Definimos las transformaciones\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    # Creamos el dataset y el dataloader\n",
    "    dataset = CustomDataset(annotations, img_dir, transform=transform)\n",
    "\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=4,\n",
    "                            shuffle=True,\n",
    "                            collate_fn=collate_fn)\n",
    "\n",
    "    # Inicializamos el modelo\n",
    "    model = ObjectDetectionModel(num_classes=num_classes)\n",
    "\n",
    "    # Entrenamos el modelo con PyTorch Lightning\n",
    "    trainer = pl.Trainer(max_epochs=2)\n",
    "    trainer.fit(model, dataloader)\n",
    "\n",
    "    # Evaluate\n",
    "    # Predict the rest of the images\n",
    "    # Upload new images to label-studio based on confidence and pre-annotate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ad8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camera_trap_workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
